{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Capstone: LSTM + CNN-GRU Time Series Notebook\n",
        "\n",
        "This notebook contains a complete pipeline to load `smart_mobility_dataset.csv` (assumed at `/mnt/data/smart_mobility_dataset.csv`), preprocess it, create sliding windows, train an LSTM and a CNN-GRU hybrid to predict `Road_Occupancy_%` (configurable), and evaluate RMSE/NRMSE. \n",
        "\n",
        "It also includes guidance on target scaling and reporting RMSE in original units.\n",
        "\n",
        "----\n",
        "⚠️ **Note:** This notebook includes `pip install` commands commented out. Run them in your environment if required (e.g., `pip install tensorflow pandas scikit-learn matplotlib`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup: uncomment and run these if packages missing\n",
        "# !pip install --upgrade pip\n",
        "# !pip install tensorflow pandas scikit-learn matplotlib seaborn\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, GRU, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "print('TensorFlow version:', tf.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset (ensure the CSV is at smart_mobility_dataset.csv)\n",
        "csv_path = 'smart_mobility_dataset.csv'\n",
        "assert os.path.exists(csv_path), f'Dataset not found at {csv_path}. Please upload there.'\n",
        "df = pd.read_csv(csv_path)\n",
        "print('Shape:', df.shape)\n",
        "display(df.head())\n",
        "display(df.describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocessing: parse timestamp, add cyclical time features, encode categories, fill NA if any\n",
        "def preprocess_df(df):\n",
        "    df = df.copy()\n",
        "    # parse timestamp\n",
        "    if 'Timestamp' in df.columns:\n",
        "        df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
        "        df = df.sort_values('Timestamp').reset_index(drop=True)\n",
        "        df['hour'] = df['Timestamp'].dt.hour\n",
        "        df['dow'] = df['Timestamp'].dt.dayofweek\n",
        "        df['is_weekend'] = df['dow'].isin([5,6]).astype(int)\n",
        "        # cyclical encodings\n",
        "        df['hour_sin'] = np.sin(2*np.pi*df['hour']/24)\n",
        "        df['hour_cos'] = np.cos(2*np.pi*df['hour']/24)\n",
        "    \n",
        "    # simple categorical encodings (one-hot for small-cardinality columns)\n",
        "    cat_cols = [c for c in ['Traffic_Light_State','Weather_Condition','Traffic_Condition'] if c in df.columns]\n",
        "    df = pd.get_dummies(df, columns=cat_cols, drop_first=True)\n",
        "    \n",
        "    # fill na numeric with forward fill then median\n",
        "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    df[num_cols] = df[num_cols].fillna(method='ffill').fillna(df[num_cols].median())\n",
        "    return df\n",
        "\n",
        "df_p = preprocess_df(df)\n",
        "print('After preprocess shape:', df_p.shape)\n",
        "display(df_p.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sliding window maker: creates (X, y) for seq-to-one forecasting\n",
        "def make_windows(df, feature_cols, target_col, seq_len=24, horizon=12):\n",
        "    X, y = [], []\n",
        "    data = df[feature_cols].values\n",
        "    targ = df[target_col].values\n",
        "    n = len(df)\n",
        "    for i in range(n - seq_len - horizon + 1):\n",
        "        X.append(data[i:i+seq_len])\n",
        "        y.append(targ[i+seq_len+horizon-1])\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    return X, y\n",
        "\n",
        "# Choose features and target (customize as needed)\n",
        "TARGET = 'Road_Occupancy_%' if 'Road_Occupancy_%' in df_p.columns else df_p.columns[0]\n",
        "FEATURES = [c for c in df_p.columns if c not in ['Timestamp', TARGET]]\n",
        "print('Target:', TARGET)\n",
        "print('Number of features:', len(FEATURES))\n",
        "\n",
        "X_all, y_all = make_windows(df_p, FEATURES, TARGET, seq_len=24, horizon=12)\n",
        "print('X shape, y shape:', X_all.shape, y_all.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train/val/test split using time-based slicing\n",
        "n = len(X_all)\n",
        "train_end = int(n * 0.7)\n",
        "val_end = int(n * 0.85)\n",
        "X_train, y_train = X_all[:train_end], y_all[:train_end]\n",
        "X_val, y_val = X_all[train_end:val_end], y_all[train_end:val_end]\n",
        "X_test, y_test = X_all[val_end:], y_all[val_end:]\n",
        "print('Train/Val/Test shapes:', X_train.shape, X_val.shape, X_test.shape)\n",
        "\n",
        "# Scaling features: fit scaler on flattened training features\n",
        "n_features = X_train.shape[2]\n",
        "feature_scaler = StandardScaler()\n",
        "X_train_flat = X_train.reshape(-1, n_features)\n",
        "feature_scaler.fit(X_train_flat)\n",
        "\n",
        "def scale_X(X, scaler):\n",
        "    s = scaler.transform(X.reshape(-1, X.shape[2])).reshape(X.shape)\n",
        "    return s\n",
        "\n",
        "X_train_s = scale_X(X_train, feature_scaler)\n",
        "X_val_s = scale_X(X_val, feature_scaler)\n",
        "X_test_s = scale_X(X_test, feature_scaler)\n",
        "\n",
        "# Scale target with separate scaler (so we can invert RMSE to original units)\n",
        "target_scaler = StandardScaler()\n",
        "y_train_s = target_scaler.fit_transform(y_train.reshape(-1,1)).reshape(-1)\n",
        "y_val_s = target_scaler.transform(y_val.reshape(-1,1)).reshape(-1)\n",
        "y_test_s = target_scaler.transform(y_test.reshape(-1,1)).reshape(-1)\n",
        "\n",
        "print('Scaling complete')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_lstm(seq_len, n_features):\n",
        "    model = Sequential([\n",
        "        LSTM(128, return_sequences=True, input_shape=(seq_len, n_features)),\n",
        "        Dropout(0.2),\n",
        "        LSTM(64),\n",
        "        Dropout(0.2),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "seq_len = X_train_s.shape[1]\n",
        "n_features = X_train_s.shape[2]\n",
        "lstm = build_lstm(seq_len, n_features)\n",
        "lstm.summary()\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(patience=8, restore_best_weights=True),\n",
        "    ReduceLROnPlateau(patience=4, factor=0.5, min_lr=1e-6)\n",
        "]\n",
        "\n",
        "# To train, uncomment below (might take time):\n",
        "# history = lstm.fit(X_train_s, y_train_s, validation_data=(X_val_s, y_val_s),\n",
        "#                   epochs=100, batch_size=64, callbacks=callbacks)\n",
        "\n",
        "# After training, to predict and invert scaling:\n",
        "# y_pred_s = lstm.predict(X_test_s).reshape(-1)\n",
        "# y_pred = target_scaler.inverse_transform(y_pred_s.reshape(-1,1)).reshape(-1)\n",
        "# from sklearn.metrics import mean_squared_error\n",
        "# rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "# print('LSTM Test RMSE:', rmse)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_cnn_gru(seq_len, n_features):\n",
        "    inp = Input((seq_len, n_features))\n",
        "    x = Conv1D(64, kernel_size=3, padding='same', activation='relu')(inp)\n",
        "    x = MaxPooling1D(pool_size=2)(x)\n",
        "    x = Conv1D(32, kernel_size=3, padding='same', activation='relu')(x)\n",
        "    x = GRU(64)(x)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    out = Dense(1)(x)\n",
        "    model = Model(inp, out)\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "cnn_gru = build_cnn_gru(seq_len, n_features)\n",
        "cnn_gru.summary()\n",
        "\n",
        "# To train, uncomment and run:\n",
        "# history2 = cnn_gru.fit(X_train_s, y_train_s, validation_data=(X_val_s, y_val_s),\n",
        "#                      epochs=100, batch_size=64, callbacks=callbacks)\n",
        "\n",
        "# Evaluate similarly by predicting, inverse-scaling, and computing RMSE in original units.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    nrmse = rmse / (y_true.max() - y_true.min())\n",
        "    return {'rmse': rmse, 'mae': mae, 'nrmse': nrmse}\n",
        "\n",
        "print('Metrics helper ready')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes / Tips\n",
        "- The notebook predicts `TARGET` at a horizon (by default horizon=12). Adjust `seq_len` and `horizon` to your use-case.\n",
        "- The notebook scales features and target separately. RMSE reported after inverse-transform is in original units.\n",
        "- If you need, I can add cross-validation, hyperparameter search (Optuna), or an automated training script.\n",
        "\n",
        "----\n",
        "Now save the notebook and open it in Jupyter/Lab to run cells interactively.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "py_version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
